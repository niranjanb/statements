\documentclass[a4paper,11pt,onecolumn]{article}
\usepackage[hscale=0.8,vscale=0.9]{geometry}
\usepackage[parfill]{parskip}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{amssymb}

\newcommand{\eat}[1]{}

\begin{document}
\include{weld-defns}
\title{Research Statement}
\author{Niranjan Balasubramanian}
\maketitle

My vision is to develop methods for extracting large scale open-domain knowledge. My research is motivated by applications such as event extraction and question answering, which require computer systems to extract, understand and reason with information present in natural language text. For example, event extraction systems use knowledge about events to guide extraction. 
\begin{wrapfigure}{r}{0.4\textwidth}
	\vspace{-2ex}
	\begin{center}
	\includegraphics[width=2.5in,height=2in]{figures/arrest-scenario} 	
	\vspace{-2ex}
	\caption{\label{fig:arrest} {\small Arrest schema: A model for an arrest scenario including key actors, the police, the suspect, judge etc. and their roles.}}
	\vspace{-2ex}
	\end{center}
\end{wrapfigure}
They use event schemas (also known as templates) to identify which pieces of information to extract about an event. For example, an arrest schema will contain the arresting agent, the person who gets arrested, and the judge who rules on the case. One of my main research focus is to automatically build such event models (also known as event schemas) from text without any human effort.

To be widely applicable, methods for constructing such semantic knowledge must meet the following design goals. They must scale to arbitrary domains, model semantics without ambiguity, and generalize to unseen contexts.
\eat{First, they must scale to arbitrary domains and hence should not rely on domain specific models or resources. Second, the representation should be specific enough to model the semantics without ambiguity. Underspecified representations allow for robust estimation but often at the expense of increased ambiguity. For instance, when modeling events, a verb only representation will conflate relations that differ by the semantics of an attached proposition (e.g, in a lawsuit scenario the object of ``file by'' is a plaintiff and ``file against'' is a defendant).\eat{Further, using a verb only representation (without prepositions) might provide robust models for more verbs but cover fewer relations.} Finally, the methods must allow for generalization to new unseen contexts. For example, a semantic model about a smart-phone must generalize to be applicable for a related but newer context of Google glass.
}
\eat{The goal of my work is to build methods that yield open domain, broad coverage semantic knowledge through careful representations that can be easily extracted using open domain technologies. My past work focused on building two types of semantic knowledge a) Rel-grams -- A form of entailment knowledge that captures what relation is likely to follow a given relation, and b) Open Schemas -- A model of the key actors and the roles they play in a scenario. }

{\bf NLP [EMNLP 2013, AKBC-WEKEX 2012, 2013]}

{\em Modeling Events}

Event schemas specify actors and their roles within events and are widely used in event extraction. Until recently, these were hand-engineered and consequently were limited to a small number of domains. My research aims to automatically generate these schemas from text with no manual effort with a specific focus on generating coherent schemas.

The main premise behind my work is that an accurate model of co-occurring actors and their actions can provide a basis for automatically generating schemas. The main challenge is in defining a suitable representation for the actions. I developed an Open IE triple-based solution that uses a (Arg1, Relation, Arg2) triple that captures more specific information about the actions and reduces ambiguity. However, this reduction in ambiguity comes at a cost of increased sparsity and reduced generalization. To counter this issue, I represent arguments using semantic classes, which allows the model to generalize beyond the specific entities to new unseen contexts. The relational co-occurrence model (Rel-grams) built over this triple representation yields a form of entailment~\cite{balasubramanian-akbc12} and produces high-quality open-domain schemas~\cite{balasubramanian-emnlp13}

{\bf Question Answering}

Achieving human-level performance on tasks that require intelligence has a long tradition in the history of Artificial Intelligence~\cite{}. As one of the foundational members of the Allen Institute for Artificial Intelligence, I am co-leading efforts to design and develop a QA system that is capable of passing a 4th grade science exam. This is an exciting long-term research that seeks to address several fundamental challenges in representation, extraction and reasoning all in the context of a challenging question answering task.

\eat{As a first step in this challenging task, we studied the knowledge requirements for passing this exam~\cite{clark-akbc13}. The analysis presented several critical insights into the challenges that lay ahead. Figure~\ref{fig:knowledge-wheel} shows the required knowledge categories as a wheel. In addition to factual knowledge (e.g., iron conducts electricity), we identify three other types of useful knowledge: 1) Definitional knowledge: Terminology questions often test the ability of students to match terminology to its description. 2) Domain knowledge: A wide variety of domain knowledge expressed via general purpose relations such as cause/effect, action/purpose, entity/function and object/property. 3) Implications: To answer many questions the system requires knowledge represented as implications (e.g, animal breathes oxygen $-$enables$\rightarrow$ animal make energy). 4) Domain models: Modeling questions involve ability to reason with certain types of models (e.g. Reasoning with predator-prey models: If population of snakes rise, what happens to the population of frogs?). }
As a first step in this challenging task, we studied the knowledge requirements for passing a 4th grade science exam~\cite{clark-akbc13}. Figure~\ref{fig:akbc} shows the required knowledge categories as a wheel. In addition to factual knowledge (e.g., iron conducts electricity), we identify three other types of useful knowledge: 1) Definitional knowledge, 2) Domain knowledge expressed via general purpose relations such as cause/effect, entity/function, 3) Implications representing domain and background knowledge (e.g., animal breathes oxygen $-$enables$\rightarrow$ animal make energy), and 4) Qualitative domain models (e.g., Reasoning with predator-prey models: If population of snakes rise, what happens to the population of frogs?).

\begin{wrapfigure}{l}{0.4\textwidth}
	\vspace{-2ex}
	\begin{center}
	\includegraphics[width=3in,height=2in]{figures/akbc} 	
	\vspace{-2ex}
	\caption{\label{fig:arrest} {\small Knowledge Requirements for passing a 4th Grade Science Exam}}
	\vspace{-2ex}
	\end{center}
\end{wrapfigure}

We are building a wide array of solutions to address these difficult challenges. We extract definitional and general purpose relations such as cause/effect and entity/function using hand-generated lexico-syntactic patterns that exploit strong regularities in language. We use Open IE style relations to represent information but expand them to cover nested relations. In addition, we also use facts extracted using a state-of-the-art Open IE extractor.

I am building a probabilistic reasoning framework that uses these knowledge sources to answer questions. Specifically, I cast question answering as a marginal inference problem in a Markov Logic Network (MLN) with implication knowledge as first-order rules and nested relations as evidence. Different from traditional applications of MLNs, the knowledge and rules here are textual, as a result of which several gaps arise due to vocabulary mismatch. To address these lexical gaps, I created an iterative bi-directional search algorithm that uses coarse but fast methods to locate plausible gaps and bridges them using deeper textual entailment techniques. One of the key challenges here is to restrict the backward-search to a feasible space. 

\eat{For acquiring definitional knowledge we exploit the strong regularities in definitional language to produce open ie style relations representing the definition. A typical definition includes the class or semantic type information about the term and one or more {\em definitional} properties. We used a handful of patterns covered 50\% of the definitions mined from the web at a high precision. For extracting domain knowledge, we use hand-generated dependency patterns to extract general purpose relations such as cause/effect. I am cure
}

{\bf Information Retrieval [SIGIR 2010, CIKM 2009, IMC 2009, CoNext 2012]}


{\bf Combining Alternatives}

The IR research community continuously develops query representations, retrieval models, and various ranking algorithms. As part of my thesis, I developed a dynamic query-dependent approach for combining different alternatives. The main premise behind my thesis is that different alternatives work well for different queries. For example, a navigation query (intent to visit a specific url) is well served by user click based features, whereas a informational query is better served by query-document match features. If we can select the best choice(s) for a given query, then we can further improve retrieval performance. 

To this end, I developed a novel method that estimates the relative performance of the alternatives with respect to a baseline using easy to compute retrieval features~\cite{balasubramanian-sigir10a}. The key insight behind this method was that accurate estimation of the absolute effectiveness value was not essential. The estimation only needed to induce a good ordering of the available alternatives. This relative estimation method outperformed using the single best alternative for query representations~\cite{balasubramanian-sigir10b} and ranking functions~\cite{balasubramanian-sigir10c}. 


{\bf Topic Pages}

As an alternative to the typical web search paradigm, I built a system that automatically generates wikipedia like pages for topics~\cite{balasubramanian-icsc2010}. The primary challenge here is to identify the salient aspects pertaining to the topic. I exploited web search logs which aggregate user interests about topics to build diverse aspect models on topics.To generalize to topics beyond those that are observed in the search logs, we generalized the aspect models to include information about related entities. A second challenge here is to extract and organize information pertaining to the diverse aspects in a coherent fashion. I built a sentence extractor that identifies most typical connection between the topic and its aspect and used simple word-precedence models to organize the retrieved sentences.

{\bf Mobile Search}

I studied the impact of system constraints on web search from mobile phones. I conducted a systematic study of how network activity consumed energy in mobile phones. My study revealed that energy consumption also depended on inter-transfer times in addition to the size of the data being transferred~\cite{balasubramanian-imc09}. Based on this insight, I designed interaction strategies that reduced the energy consumption of web search applications. The key finding in this work spawned a large body of research aimed at minimizing the energy inefficiency. 

I also worked on a local mobile search system that leveraged re-finding behavior of web search users to reduce repeat downloads~\cite{balasubramanian-conext12}. Because indexing locally is expensive, the key challenge in this setting is to figure out which documents to index. The system must balance local availability against resource usage and energy consumption. Using actual search logs from mobile users, we learned user-specific re-finding patterns to predict when a user is likely to re-find documents. Using this predictive model, we built an adaptive search system that provided an effective and energy efficient local search system.

{\bf Future Work}

I am interested in extracting open-domain semantic knowledge from text. In particular I wish to exploit existing semantic resources and extraction capabilities to build richer semantic structures at scale. I am also interested in developing robust application mechanisms to handle the gaps and noise inherent in such automatically extracted knowledge.

{\bf Script-like Knowledge}

Script like knowledge provide powerful models of events at discourse level by including key actors, their roles and causal relationships~\cite{schank-scripts75}. Rel-grams and schemas provide a start. However, their generalization is somewhat limited due to use of a limited set of semantic classes chosen in an ad-hoc fashion. Further, they do not include other valuable discourse relations such as causality and temporal ordering. First, I wish to exploit high-quality entity linking systems and semantic resources such as NELL to expand to a much larger hierarchy of semantic classes. This raises interesting challenges. For example, it is difficult to decide what semantic class to assign to a specific relation. In some cases a general category is preferable (e.g., ([sportsperson], failed, [drug test]) ), whereas in others a more specific category is preferable (e.g., ([baseball player], scored, home run)).  Second, I wish to leverage causality detection methods to annotate the schemas with causal relations. Even though these methods are not perfect, we can devise techniques that can prune errors in aggregation. 

Scalable event extraction requires open domain models of events in terms of the actors and their roles. Past attempts have either used manual templates~\cite{patwardhan-emnlp09} or have used probabilistic methods that limit application to large scale open domain data~\cite{cheung-naacl13,chambers-emnlp13}. Open event schemas are coherent models of the key actors and their roles and thus provide a strong basis constructing high-quality extractors at scale. Expanding schemas to include extractors can be viewed as a bootstrapping procedure starting from a strong high-quality model of an event. As with any bootstrapping approach the challenges are in finding appropriate sources for expansion and avoiding drifting from the source model. 

Resolving co-reference is a challenging problem that requires broad semantic knowledge in the form of type constraints. Consider the following sentence: ``[People] travel to see their [families] when [they] find cheap flights to take [them]". [they] can resolve to families or [people]. Resolving pronouns in this sentence requires the system to know that the entity that travels is the one that is likely to find flights. Rel-grams provides a framework for aggregating this kind of background knowledge. One of the key challenges is the precision/recall trade-off. Past attempts at improving co-reference by adding semantics have mixed results because they either lack coverage or are too imprecise due to over-generalization. Rel-grams addresses this to some extent by including more context to improve accuracy and semantic classes to improve generalization.

{\bf Question Answering}

Passing grade science exams requires extraction and reasoning with implications knowledge. The work on Rel-grams provides a starting point but is seriously limited especially in terms of the amount of context captured. A large portion of this knowledge requires using complex nested structures for representation, which makes the inference problem harder. I will explore hybrid architectures which allow for reasoning with representations with varying degrees of complexity to avoid inference with complex structures where possible. 

Reasoning with semantic resources constructed from texts requires robust approaches. However, past attempts at producing robust approaches has mainly resulted in everything in the kitchen sink style approaches. This has led to measured progress in textual entailment tasks, but they haven't shed much insights into the properties of the systems themselves or on the problem space. In contrast, I am interested in approaches that preserve the deductive style of inference as much as possible while falling back to distributional methods to bridge gaps but within the inference framework. This I believe combines the best of both worlds -- retains the robustness of the entailment style methods while also explicitly showing where the current knowledge is lacking or where textual reasoning is required. The question answering project is a great testbed to test these ideas.

{\bf Information Retrieval}

Lastly, I am interested in exploiting semantic knowledge for Information retrieval. In the past, IR applications have had mixed success with using semantic resources. The key limiting factor was the coverage of the resources used and scalability of the methods. Recent advances in large scale language processing and knowledge extraction techniques provide an ideal opportunity to test integration of semantics. Open Information Extraction presents an ideal starting point. It provides fast and a shallow representation of salient information in a corpus that can be used to improve retrieval. 


\eat{
%Resolving co-reference is a challenging problem that requires broad semantic knowledge~\cite{}. Consider the following sentence: ``[People] travel to see their [families] when [they] find cheap flights to take [them]". [they] can resolve to families or [people]. Resolving pronouns in this sentence requires the system to know that the entity that travels is the one that is likely to find flights. Rel-grams provides a framework for aggregating this kind of background knowledge. One of the key challenges is the precision/recall trade-off. Past attempts at improving co-reference by adding semantics have mixed results because they either lack coverage or are too imprecise due to over-generalization. Rel-grams addresses this to some extent by including more context to improve accuracy and semantic classes to improve generalization. [But more needs to be done... What?]

Question answering for passing grade science exams requires extraction and reasoning with implications knowledge. The work on Rel-grams provides a starting point but is seriously limited especially in terms of the amount of context captured. For example, consider the question about 
...\\
...\\
...\\


Reasoning with semantic resources constructed from texts requires robust approaches. However, past attempts at producing robust approaches has mainly resulted in everything in the kitchen sink style approaches. This has led to measured progress in textual entailment tasks, but they haven't shed much insights into the properties of the systems themselves or on the problem space. In contrast, I am interested in approaches that preserve the deductive style of inference as much as possible while falling back to distributional methods to bridge gaps but within the inference framework. This I believe combines the best of both worlds -- retains the robustness of the entailment style methods while also explicitly showing where the current knowledge is lacking or where textual reasoning is required. The question answering project is a great testbed to test these ideas...\\

Lastly, I am interested in exploiting semantic resources for Information retrieval. In the past, IR applications have had mixed success with using semantic resources. The key limiting factor was the coverage of the resources used...\\
}
{\small
\bibliographystyle{plain}
\bibliography{../research-statement,../kia}
}
\end{document}
